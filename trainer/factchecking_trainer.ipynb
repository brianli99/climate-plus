{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForMaskedLM\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 914/914 [00:00<00:00, 435kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/codespace/.cache/huggingface/datasets/rexarski___parquet/rexarski--climate_fever_fixed-967e3bdb8fd2c62b/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 763k/763k [00:00<00:00, 41.1MB/s]\n",
      "Downloading data: 100%|██████████| 279k/279k [00:00<00:00, 55.4MB/s]]\n",
      "Downloading data: 100%|██████████| 331k/331k [00:00<00:00, 29.4MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00,  4.46it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1203.88it/s]\n",
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/codespace/.cache/huggingface/datasets/rexarski___parquet/rexarski--climate_fever_fixed-967e3bdb8fd2c62b/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 376.16it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('rexarski/climate_fever_fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_dict_list = [{k: v for k, v in item.items()} for item in ds[\"train\"]]\n",
    "train_df = pd.DataFrame(train_split_dict_list)\n",
    "\n",
    "test_split_dict_list = [{k: v for k, v in item.items()} for item in ds[\"test\"]]\n",
    "test_df = pd.DataFrame(test_split_dict_list)\n",
    "\n",
    "val_split_dict_list = [{k: v for k, v in item.items()} for item in ds[\"valid\"]]\n",
    "val_df = pd.DataFrame(val_split_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim_id     int64\n",
      "claim       object\n",
      "evidence    object\n",
      "label        int64\n",
      "category    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.dtypes)\n",
    "\n",
    "train_df['claim'] = train_df['claim'].astype(str)\n",
    "train_df['evidence'] = train_df['evidence'].astype(str)\n",
    "\n",
    "val_df['claim'] = val_df['claim'].astype(str)\n",
    "val_df['evidence'] = val_df['evidence'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_text(df, threshold=512):\n",
    "    df = df.assign(claim_length=df['claim'].apply(lambda x: len(x)))\n",
    "    df = df.assign(evidence_length=df['evidence'].apply(lambda x: len(x)))\n",
    "    df = df.assign(total_length=lambda x: x['claim_length'] + x['evidence_length']).sort_values('total_length', ascending=False)\n",
    "    df = df[df['total_length'] <= threshold]\n",
    "    return df\n",
    "\n",
    "train_df = trim_text(train_df, 128)\n",
    "val_df = trim_text(val_df, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>claim_length</th>\n",
       "      <th>evidence_length</th>\n",
       "      <th>total_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2083</td>\n",
       "      <td>Climategate CRU emails suggest conspiracy</td>\n",
       "      <td>\"'Conspiracy theories finally laid to rest' by...</td>\n",
       "      <td>1</td>\n",
       "      <td>Phil Jones (climatologist)</td>\n",
       "      <td>41</td>\n",
       "      <td>87</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4165</th>\n",
       "      <td>2168</td>\n",
       "      <td>IPCC graph showing accelerating trends is misl...</td>\n",
       "      <td>The IPCC needs to look at this trend in the er...</td>\n",
       "      <td>0</td>\n",
       "      <td>Intergovernmental Panel on Climate Change</td>\n",
       "      <td>52</td>\n",
       "      <td>76</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>2401</td>\n",
       "      <td>Mother Earth has clearly ruled that CO2 is not...</td>\n",
       "      <td>Its soil is utterly barren and its atmosphere ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Earth in science fiction</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>2330</td>\n",
       "      <td>Arctic sea ice has been retreating over the pa...</td>\n",
       "      <td>\"What drove the dramatic arctic sea ice retrea...</td>\n",
       "      <td>2</td>\n",
       "      <td>Global warming</td>\n",
       "      <td>58</td>\n",
       "      <td>69</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>1014</td>\n",
       "      <td>Final data for 2016 sea level rise have yet to...</td>\n",
       "      <td>Between 1900 and 2016, the sea level rose by 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sea level rise</td>\n",
       "      <td>60</td>\n",
       "      <td>67</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      claim_id  ... total_length\n",
       "8         2083  ...          128\n",
       "4165      2168  ...          128\n",
       "753       2401  ...          128\n",
       "1609      2330  ...          127\n",
       "1045      1014  ...          127\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151, 8)\n",
      "(64, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"distilroberta-base\"\n",
    "model_checkpoint = \"climatebert/distilroberta-base-climate-f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class climateFever(Dataset):\n",
    "\n",
    "    def __init__(self, train_df, val_df):\n",
    "        self.label_dict = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT_ENOUGH_INFO\": 2}\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "\n",
    "        self.base_path = '/content/'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model, do_lower_case=True)\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        self.init_data()\n",
    "\n",
    "    def init_data(self):\n",
    "        self.train_data = self.load_data(self.train_df)\n",
    "        self.val_data = self.load_data(self.val_df)\n",
    "\n",
    "    def load_data(self, df):\n",
    "        MAX_LEN = 512\n",
    "        token_ids = []\n",
    "        mask_ids = []\n",
    "        seg_ids = []\n",
    "        y = []\n",
    "\n",
    "        premise_list = df['claim'].to_list()\n",
    "        hypothesis_list = df['evidence'].to_list()\n",
    "        label_list = df['label'].to_list()\n",
    "\n",
    "        for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n",
    "            premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
    "            hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
    "            pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "            premise_len = len(premise_id)\n",
    "            hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "            segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))\n",
    "            attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))\n",
    "\n",
    "            token_ids.append(torch.tensor(pair_token_ids))\n",
    "            seg_ids.append(segment_ids)\n",
    "            mask_ids.append(attention_mask_ids)\n",
    "            # y.append(self.label_dict[label])\n",
    "            y.append(label)\n",
    "\n",
    "        token_ids = pad_sequence(token_ids, batch_first = True)\n",
    "        mask_ids = pad_sequence(mask_ids, batch_first = True)\n",
    "        seg_ids = pad_sequence(seg_ids, batch_first = True)\n",
    "        y = torch.tensor(y)\n",
    "        dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "        print(len(dataset))\n",
    "        return dataset\n",
    "    \n",
    "    def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_data,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            self.val_data,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "\n",
    "    # def __init__(self, ds, base_model):\n",
    "    #     self.label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
    "\n",
    "    #     # self.train_df = ds[\"train\"]\n",
    "    #     # self.val_df = ds[\"valid\"]\n",
    "    #     # # or:\n",
    "    #     train_split_dict_list = [{k: v for k, v in item.items()} for item in ds[\"train\"]]\n",
    "    #     val_split_dict_list = [{k: v for k, v in item.items()} for item in ds[\"valid\"]]\n",
    "    #     self.train_df = pd.DataFrame(train_split_dict_list)[:500]\n",
    "    #     self.val_df = pd.DataFrame(val_split_dict_list)[:100]\n",
    "        \n",
    "    #     self.base_path = \"/content/\"\n",
    "    #     self.tokenizer = AutoTokenizer.from_pretrained(base_model, do_lower_case=True)\n",
    "    #     self.train_data = None\n",
    "    #     self.val_data = None\n",
    "    #     self.init_data()\n",
    "\n",
    "    # def init_data(self):\n",
    "    #     self.train_data = self.load_data(self.train_df)\n",
    "    #     self.val_data = self.load_data(self.val_df)\n",
    "    \n",
    "    # def load_data(self, df):\n",
    "    #     MAX_LEN = 512\n",
    "    #     token_ids = []\n",
    "    #     mask_ids = []\n",
    "    #     seg_ids = []\n",
    "    #     y = []\n",
    "\n",
    "    #     premise_list = df[\"claim\"]#.to_list()\n",
    "    #     hypothesis_list = df[\"evidence\"]#.to_list()\n",
    "    #     label_list = df[\"label\"]#.to_list()\n",
    "\n",
    "    #     for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n",
    "    #         premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
    "    #         hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
    "    #         pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "            \n",
    "    #         premise_len = len(premise_id)\n",
    "    #         hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "    #         segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "    #         attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "    #         token_ids.append(torch.tensor(pair_token_ids))\n",
    "    #         seg_ids.append(segment_ids)\n",
    "    #         mask_ids.append(attention_mask_ids)\n",
    "    #         y.append(self.label_dict[label])\n",
    "        \n",
    "    #     token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    #     mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    #     seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    #     y = torch.tensor(y)\n",
    "    #     dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "    #     print(len(dataset))\n",
    "    #     return dataset\n",
    "    \n",
    "    # def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "    #     train_loader = DataLoader(\n",
    "    #         self.train_data,\n",
    "    #         shuffle=shuffle,\n",
    "    #         batch_size=batch_size\n",
    "    #     )\n",
    "\n",
    "    #     val_loader = DataLoader(\n",
    "    #         self.val_data,\n",
    "    #         shuffle=shuffle,\n",
    "    #         batch_size=batch_size\n",
    "    #     )\n",
    "\n",
    "    #     return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "# processed_dataset = climateFever(ds, base_model)\n",
    "processed_dataset = climateFever(train_df, val_df)\n",
    "train_loader, val_loader = processed_dataset.get_data_loaders(batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 752/752 [00:00<00:00, 337kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 329M/329M [00:04<00:00, 77.5MB/s] \n",
      "Some weights of the model checkpoint at climatebert/distilroberta-base-climate-f were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50500, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
    "# model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, num_labels=3)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m param_optimizer \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39mnamed_parameters())\n\u001b[1;32m      2\u001b[0m no_decay \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbeta\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m optimizer_grouped_parameters \u001b[39m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m: [p \u001b[39mfor\u001b[39;00m n, p \u001b[39min\u001b[39;00m param_optimizer \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(nd \u001b[39min\u001b[39;00m n \u001b[39mfor\u001b[39;00m nd \u001b[39min\u001b[39;00m no_decay)],\n\u001b[1;32m      5\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39mweight_decay_rate\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.01\u001b[39m},\n\u001b[1;32m      6\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m: [p \u001b[39mfor\u001b[39;00m n, p \u001b[39min\u001b[39;00m param_optimizer \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(nd \u001b[39min\u001b[39;00m n \u001b[39mfor\u001b[39;00m nd \u001b[39min\u001b[39;00m no_decay)],\n\u001b[1;32m      7\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39mweight_decay_rate\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.0\u001b[39m},\n\u001b[1;32m      8\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay_rate\": 0.01},\n",
    "    {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay_rate\": 0.0},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "def multi_acc(y_pred, y_test):\n",
    "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer):\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_acc = 0\n",
    "        for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"pair_token_ids shape: {pair_token_ids.shape}\")\n",
    "            print(f\"mask_ids shape: {mask_ids.shape}\")\n",
    "            print(f\"seg_ids shape: {seg_ids.shape}\")\n",
    "            print(f\"y shape: {y.shape}\")\n",
    "            pair_token_ids = pair_token_ids.to(device)\n",
    "            mask_ids = mask_ids.to(device)\n",
    "            seg_ids = seg_ids.to(device)\n",
    "            labels = y.to(device)\n",
    "            # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "            loss, prediction = model(pair_token_ids, token_type_ids=seg_ids, attention_mask=mask_ids, labels=labels).values()\n",
    "\n",
    "            # loss = criterion(prediction, labels)\n",
    "            acc = multi_acc(prediction, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_acc += acc.item()\n",
    "\n",
    "        train_acc = total_train_acc / len(train_loader)\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "        model.eval()\n",
    "        total_val_acc = 0\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
    "                optimizer.zero_grad()\n",
    "                pair_token_ids = pair_token_ids.to(device)\n",
    "                mask_ids = mask_ids.to(device)\n",
    "                seg_ids = seg_ids.to(device)\n",
    "                labels = y.to(device)\n",
    "\n",
    "                # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "                loss, prediction = model(pair_token_ids, token_type_ids=seg_ids, attention_mask=mask_ids, labels=labels).values()\n",
    "\n",
    "                # loss = criterion(prediction, labels)\n",
    "                acc = multi_acc(prediction, labels)\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                total_val_acc += acc.item()\n",
    "\n",
    "        val_acc = total_val_acc / len(val_loader)\n",
    "        val_loss = total_val_loss / len(val_loader)\n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end - start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}\")\n",
    "        print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes), seconds))\n",
    "\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Example\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"amandakonet/climatebert-fact-checking\", use_auth_token=True\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"amandakonet/climatebert-fact-checking\", use_auth_token=True\n",
    "# )\n",
    "\n",
    "# features = tokenizer(\n",
    "#     [\"Beginning in 2005, however, polar ice modestly receded for several years\"],\n",
    "#     ['Polar Discovery \"Continued Sea Ice Decline in 2005'],\n",
    "#     padding=\"max_length\",\n",
    "#     truncation=True,\n",
    "#     return_tensors=\"pt\",\n",
    "#     max_length=512,\n",
    "# )\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     scores = model(**features).logits\n",
    "#     label_mapping = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "#     labels = [label_mapping[score_max] for score_max in scores.argmax(dim=1)]\n",
    "#     print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
